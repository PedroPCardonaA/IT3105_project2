# MuZero Training Configuration
# Default configuration for MuZero training with MCTS

# Environment settings
environment:
  name: breakout  # Options: breakout, asterix, freeway, seaquest, space_invaders
  seed: 42
  max_steps_per_episode: 624

# Network architecture
network:
  hidden_dim: 128  # Hidden state dimension
  obs_channels_multiplier: 1  # Will be multiplied by (C * history_len)

# Training hyperparameters
training:
  num_episodes: 3000  # Longer training budget (you said you can train a lot)
  history_len: 8  # Number of frames to stack (q+1 look-back window)
  unroll_steps: 5  # Number of steps to unroll (w roll-ahead actions)
  td_steps: 10  # TD-lambda steps for value target
  discount: 0.997  # Gamma discount factor (matches MCTS)
  batch_size: 64  # Increased from 32 for more stable gradients
  learning_rate: 0.0005  # 5e-4 (reduced from 1e-3)
  weight_decay: 0.0001  # 1e-4 (increased for regularization)
  grad_clip: 10.0  # Gradient clipping norm
  
  # Loss weights
  policy_loss_weight: 1.0
  value_loss_weight: 1.0
  reward_loss_weight: 1.0
  
  # Training schedule
  # MCTS is the main cost driver, so we avoid training every single episode.
  train_interval_episodes: 2  # Train every N episodes
  min_buffer_episodes: 20  # Let buffer diversify before training
  train_batches_per_interval: 50  # Fewer batches per interval to keep wall-time reasonable

# MCTS settings
mcts:
  # Big speed/perf lever: these two control most of the compute.
  # 50x8 is usually a much better training-time tradeoff than 100x10.
  num_simulations: 50  # Number of MCTS simulations per move
  max_depth: 8  # Maximum search depth
  pb_c_base: 19652  # UCB constant base
  pb_c_init: 1.25  # UCB constant init
  dirichlet_alpha: 0.25  # Dirichlet noise alpha
  root_exploration_frac: 0.25  # Fraction of root prior from Dirichlet noise

# Replay buffer
buffer:
  capacity: 200  # Maximum number of episodes to store

# Action selection
action_selection:
  temperature: 1.0  # Initial temperature for action sampling (lower = more greedy)
  # Decay a bit slower than before to keep exploration longer.
  temperature_decay: 0.997  # Multiply temperature by this after each episode
  min_temperature: 0.1  # Minimum temperature (prevents too greedy early)

# Checkpointing and results
checkpointing:
  save_interval_episodes: 100  # Save model checkpoint every N episodes
  results_dir: muzero_results  # Base directory for saving results

# Logging
logging:
  print_every: 1  # Print stats every N episodes
  verbose: true
